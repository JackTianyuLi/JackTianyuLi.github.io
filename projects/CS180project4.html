<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CS180 Project 4</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <link rel="stylesheet" href="../style.css">
 
</head>
<div class="container">
<body>
  <header>
    <h1>CS180 Project 4</h1>
  </header>

  <h2 id="Part-0">Part 0: Camera Calibration and 3D Scanning</h2>
  <p>
    In this <a href="https://cal-cs180.github.io/fa25/hw/proj4/index.html" target="_blank">part of the project</a>, 
    we are required to conduct the full pipeline of Neural Radiance Field(NeRF). Here, I took 33 photos of an object
    from different viewpoints with distance 10 ~ 20cm. After calibrating the intrinsics of my phone camera,
    I got the following intrinsics:
      \[
      \begin{bmatrix}
      f_x & 0 & c_x \\
      0 & f_y & c_y \\
      0 & 0 & 1
      \end{bmatrix}
      \]
      where \(f_x = 2775.42\) and \(f_y=2773.99\) are the focal lengths in the x and y directions, respectively, and \(c_x=1950\) and \(c_y=1454\) are the 
      principal points in the x and y directions, respectively.
    Then, with the images, I derived the camera's extrinsic parameters for each of them. Here are the 
    visualizations of them:
  </p>
  <div style="display: flex; justify-content: center; margin: 0 auto; margin-top: 2rem; margin-bottom: 2rem; max-width: 1200px;">
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/viser1.png" alt="Part01" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
          View from left
        </figcaption>
    </figure>
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/viser2.png" alt="Part02" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
          View from behind
        </figcaption>
    </figure>
  </div>

  <h2 id="Part-1">Part 1: Fit a Neural Field to a 2D Image</h3>
  <p>
  In this part, I started by fitting MLPs to two images shown below:   
  </p>
  <div style="display: flex; justify-content: center; gap: 1px; margin: 0 auto; margin-top: 2rem; margin-bottom: 2rem; max-width: 1000px;">
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/test_img1.jpg" alt="Part11" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
          Test image 1
        </figcaption>
    </figure>
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/test_img2.jpg" alt="Part12" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
          Test image 2
        </figcaption>
    </figure>
  </div>
  <p>
  In the MLPs, positional encodings(PE) are added before the fully connected layers.
  To fit them, I used the following training settings and derived the following results:
  </p>
  <table border="1" cellspacing="0">
  <thead>
    <tr>
      <th scope="col">Image index</th>
      <th scope="col">Image size</th>
      <th scope="col">Training epochs</th>
      <th scope="col">Learning rate</th>
      <th scope="col">PE frequencies</th>
      <th scope="col">Hidden features</th>
      <th scope="col">Hidden layera</th>
      <th scope="col">Batch Size</th>
      <th scope="col"><strong>Test PSNR</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">1</th>
      <td>1024*689</td>
      <td>6000</td>
      <td>1e-3</td>
      <td>10</td>
      <td>512</td>
      <td>4</td>
      <td>4096</td>
      <td><strong>30.56 dB</strong></td>
    </tr>
    <tr>
      <th scope="row">2</th>
      <td>3904*2928</td>
      <td>6000</td>
      <td>2e-3</td>
      <td>10</td>
      <td>512</td>
      <td>4</td>
      <td>8192</td>
      <td><strong>35.12 dB</strong></td>
    </tr>
  </tbody>
  </table>
  <p>
  Here are the visualizations of the training progress and PNSR curve of test image 1 are shown:
  </p>
  </p>
  <div style="display: flex; justify-content: center; gap: 1px; margin: 0 auto; margin-top: 2rem; margin-bottom: 2rem; max-width: 1000px;">
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/test_image1.gif" alt="Part13" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
          Test image 1
        </figcaption>
    </figure>
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/test_image2.gif" alt="Part14" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
          Test image 2
        </figcaption>
    </figure>
  </div>
  <p>
  <div style="display: flex; justify-content: center; gap: 1px; margin: 0 auto; margin-top: 2rem; margin-bottom: 2rem; max-width: 650px;">
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/psnr_curve1.png" alt="Part15" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
          PSNR curve of test image 1
        </figcaption>
    </figure>
  </div>
  <p>
  Also, there are ablation studies on the effect of network width and positional encoding frequency on PSNR of image 1:
  </p>
  <table border="1" cellspacing="0">
  <thead>
    <tr>
      <th scope="col">Hidden features</th>
      <th scope="col">Freq = 10</th>
      <th scope="col">Freq = 30</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>128</td>
      <td>26.63</td>
      <td>26.45</td>
    </tr>
    <tr>
      <td>512</td>
      <td>30.56</td>
      <td>30.17</td>
    </tr>
  </tbody>
</table>

  <p><strong>Summary:</strong> Increasing the network width from 128 to 512 significantly improves the reconstruction quality, 
  raising PSNR from roughly 26.5 dB to over 30 dB. However, increasing the positional encoding frequency from 10 to 30 
  yields little improvement and may slightly reduce stability, suggesting that a moderate frequency (around 10) is 
  sufficient for this image.</p>

  <h2 id="Part-2">Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>
 
  <h3 id="part-B1">Train with the Lego dataset</h3>
  <p>
    In this section, I implemented the following functions to transform between camera and world coordinates:
  </p>
    <h4>1. <code>transform(c2w, x_c)</code></h4>
  <p>
    Converts camera coordinates to world coordinates.
    Each camera-to-world matrix (<code>c2w</code>) is applied to the homogeneous pixel points <code>x_c</code>,
    adding a homogeneous 1 and multiplying by the 4*4 extrinsic matrix to obtain world positions.
    Broadcasting enables batch processing of multiple rays or cameras.
  </p>

  <h4>2. <code>pixel_to_camera(K, uv, s)</code></h4>
  <p>
    Transforms 2D pixel coordinates (<code>u, v</code>) into 3D camera-space coordinates.
    It inverts the intrinsic matrix <code>K</code> and multiplies it by the homogeneous pixel coordinates,
    scaling by the depth factor <code>s</code>.
    This defines where each pixel lies in the camera coordinate frame.
  </p>

  <h4>3. <code>pixel_to_ray(K, c2w, uv)</code></h4>
  <p>
    Computes the <strong>origin</strong> and <strong>direction</strong> of camera rays.
    It first converts pixel positions to camera space using <code>pixel_to_camera</code>,
    then transforms them to world space using <code>transform</code>.
    The ray direction is normalized as:
  </p>
  <pre><code>r_d = (x_w - o) / ||x_w - o||</code></pre>
  <p>
    where <code>o</code> is the camera center from <code>c2w</code>.
  </p>

  <h4>4. <code>sample_along_rays(rays_o, rays_d, near, far, n_samples, perturb)</code></h4>
  <p>
    Uniformly samples 3D points along each ray between <code>near</code> and <code>far</code> bounds.
    Random perturbation can be added for anti-aliasing.
    Returns the 3D sample positions and their corresponding depth values <code>t_i</code>.
  </p>

  <h4>5. <code>RaysData</code> class</h4>
  <p>
    Loads all training images, camera intrinsics (<code>K</code>), and extrinsics (<code>c2w</code>).
    Provides two ray sampling modes:
  </p>
  <ul>
    <li><code>sample_rays</code>: samples rays across multiple random images.</li>
    <li><code>sample_rays_single</code>: samples rays from one random image.</li>
  </ul>
  <p>
    Each method returns a tuple <code>(ray_o, ray_d, pixel_color)</code>
    used for batched NeRF training.
  </p>
    <p>
      The following figures show the samples and rays from multiple/one camera views:
    </p>
    <div style="display: flex; justify-content: space-around; margin-top: 2rem; margin-bottom: 2rem;">
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/viser3.png" alt="part21" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
          Multi-view samples
        </figcaption>
    </figure>
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/viser4.png" alt="part22" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
          Single-view samples
        </figcaption>
    </figure>
  </div>
  <p>
    With such procedure and the training settings identical to the suggested in the project description, 
    I obtained the following results with validation PSNR 23.35 dB:
  </p>
  <div style="display: flex; justify-content: center; gap: 1px; margin: 0 auto; margin-top: 2rem; margin-bottom: 2rem; max-width: 650px;">
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/psnr_curve2.png" alt="part23" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
          PSNR curve on Lego dataset
        </figcaption>
    </figure>
  </div>
  <div style="display: flex; justify-content: center; gap: 1px; margin: 0 auto; margin-top: 2rem; margin-bottom: 2rem; max-width: 800px;">
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/lego_progress.gif" alt="part24" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
          Training progress on Lego dataset
        </figcaption>
    </figure>
    <figure style="flex: 1; text-align: center;">
    <video src="./CS180_4/video1.mp4" 
           controls 
           loop 
           autoplay 
           muted 
           style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        Your browser does not support the video tag.
    </video> 
    <figcaption style="font-size: 0.85rem; color: #2e324d;">
        Novel view rendering of Lego dataset
    </figcaption>
 </figure>
  </div>
  <p>

  </p>
  
  <h3 id="part-B2">Training with my own dataset</h3>
  <p>
    To conduct experiment on a different dataset, I made the adjustment on hyperparameter,
    with the new setting having <code>lr=1e-3, num_epochs=10000, batch_size=8192, num_samples = 128</code>
    Here are the training progress and derived novel view rendering results:
  </p>
  <div style="display: flex; justify-content: center; gap: 1px; margin: 0 auto; margin-top: 2rem; margin-bottom: 2rem; max-width: 650px;">
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/loss_curve.png" alt="part23" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
         Loss curve on my own dataset
        </figcaption>
    </figure>
  </div>
  <div style="display: flex; justify-content: center; gap: 1px; margin: 0 auto; margin-top: 2rem; margin-bottom: 2rem; max-width: 1000px;">
    <figure style="flex: 1; text-align: center;">
        <img src="./CS180_4/kita_progress.gif" alt="part24" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
        <figcaption style="font-size: 0.85rem; color: #2e324d;">
          Training progress on my own dataset
        </figcaption>
    </figure>
    <figure style="flex: 1; text-align: center;">
    <img src="./CS180_4/render3.gif" alt="part24" style="width: 100%; height: auto; display: block; margin-bottom: 10px; border-radius: 3px;">
    <figcaption style="font-size: 0.85rem; color: #2e324d;">
        Novel view rendering of my own dataset, <br>
        see the video <a href="https://www.youtube.com/watch?v=xFqUwZBiDXg">here</a>.
 </figure>


</body>
</div>

</html>
